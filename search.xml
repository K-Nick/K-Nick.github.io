<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[#CMU15-213# Everything We Need]]></title>
    <url>%2F2019%2F02%2F09%2FCMU15-213_L0%2F</url>
    <content type="text"><![CDATA[Everything We Need CMU15-213 对应课程资料整合 这门课使用的教材为CSAPP 3th 神书配神课，据说这是CMU的镇校课之一… 愣是被我到处嫖凑齐了资料 总之打算长期给学完了，毕竟计算机原理是后期很多课的基础 比如操作系统 ,计算机网络 等等…好吧，可能计算机网络和我们专业更接近一点 站点资料 包括课程资料和博客参考 课程首页 这里选用2018 spring的课程主页 主要是课件比较齐全，包括习题课(recitation)讲义和每节课的lecture 以及每节课可以与ppt同步的video~ 如果需要的话可以在这里补齐 这里顺便解释一下recitation和tutorial是啥…//摘自知乎 Recitation是复习课，通常是高年级研究生来上。极少数情况下也会有讲师或者教授亲自主讲。只有非常重要的基础课程才会享受后者的待遇。Recitation是课堂内容的总结和延伸。 与之相关的概念是Tutorial，也就是习题课。一般是研究生助教上，以做题目为主。 博客参考 这是一个在CMU读完master的大神的博客 包括 每个章节的核心解读 以及 lab解析 //lab解析是最棒的有没有！ 以及我真的难以想象他是怎么做到看书这么快还能写博文的… 课程视频 youtube上有，其实建议直接可以看18 spring的官方给的video //真是诡异了，其他资料都是不让外校访问唯独这里可以… 文档资料 CSAPP 3th 干脆就直接读的原版，反正每一章节都有指定的导读，一次最多也就两三节。 Syllabus 这个很重要，包括导读以及学习的提纲，比如哪几节学完对应哪个lab，有利于系统学习。 lab 这门课的精髓，一定要自己把代码敲完！ 可以参考的资料有 lab-handout(code) README recitation 博客中lab部分的“题意解读”和“lab解答” lecture 很不幸，全都是PPT！不像CS161每个lecture好歹配个note… 不过还好，反正有书。 不过好有残念啊…过两天再找找]]></content>
      <categories>
        <category>计算机原理</category>
      </categories>
      <tags>
        <tag>CMU15-213</tag>
        <tag>Data Integration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# ProblemSet1 Solution]]></title>
    <url>%2F2019%2F02%2F07%2FCS161_P1%2F</url>
    <content type="text"><![CDATA[PoblemSet1 Finished Solution 这里完成以下ProblemSet 1 对应章节大概是L1, L2 Exercise 看完了… 取 $n_0=5, c=8$ ,满足$n&gt;n_0$时，$2\sqrt{n}+6&lt;8\sqrt{n}$ A) 显然是 $O(n)​$。for循环总共n次，每次进行判断（以及一次赋值），但常数不重要，所以1op和2op不区分 B) 取 $n_0=20000, c=0.0001​$ 。 原lst长度太小了，无法体现渐近，调整到了万级 C) 通过绘图的最远两个点大概估计了一下斜率。据此可算出 $n=10^{15}$ 时大概需要花的时间 a) $T(n)=2\sqrt{n}+6,\ g(n)=\sqrt{n}​$ 取 $n_0=2, c=8​$，显然满足 $\forall n&gt;2，2\sqrt{n}+6&lt;8\sqrt{n}​$ $\therefore 2\sqrt{n}+6=O(\sqrt n)​$ b) $T(n)=n^2, \ g(n)=n$ 取 $n_0=2, c=1$, 显然满足 $\forall n&gt;2, 2n^2&gt;n$ $\therefore n^2=\Omega(n)$ c) $T(n)=\log_2{n},\ g(n)=\ln(n)$ 取 $n_0=1, c_1=\log_2(e-1),c_2=\log_2(e+1)$ , 显然满足 $\forall n&gt;1,\ \log_2(e-1)\ln{n}&lt;log_2n&lt;\log_2(e+1)\ln n$ $\therefore \log_2(n) = \Theta(\ln n)$ d) 假设 $4^n = O(2^n)​$ 那么应有 $\forall n &gt; n_0, 4^n &lt; c 2^n​$ ,也就是 $c &gt; 2^n​$ 但是对于任一 $n_0​$ ，满足这个条件的常数 $c​$ 都不存在 $\therefore 4^n \neq O(2^n)​$ Problems (a)和(b)逻辑是没问题的，但是©是错的。© 最致命的一点是把$O(1)$ 次&quot;操作&quot;和$O(1)$ 的时间效率混淆。那按这么说，推到极端，所有程序都是$O(1)$ 的，因为他们只调用了一次main。 题意简单而言，这题就是有n只青蛙，有好蛙和坏蛙，让他们相互评价，一次评价叫一次”toad-to-toad comparison“(此处因政治因素不对toad做直译，下称互评)。最终要求通过 $O(n)$ 次互评判断所有好坏。 a) $O(n^2)$ 的算法相当简单，对于任意一青蛙，让其与其他n-1只青蛙进行互评即可。 假设你是坏蛙，那么剩下至少一半的好蛙一定会说你是坏蛙。 假设你是好蛙，那么剩下至少一半的好蛙一定会说你是好蛙。 结果就是，”大多数蛙的判断是对的。“ b) 这里要求的函数功能是： 通过至多n/2次互评找出一个大小为m(m至多为$n/2​$)的子集 这个子集至少有一半是好蛙。 给我们n/2次操作，那不难相当分成两组对应互评这一策略，问题是怎样通过结果找到这个子集呢？ 我们可以建立一个逆向真值表，也就是通过互评结果看看有哪几种可能 互评结果 实际可能 TT TT or FF TF FT or FF FT TF or FF FF TF or FT or FF 可以看出，只有在结果为TT时，我们从{A, B}中取出的A与B的关系是确定的，要么都是好蛙，要么都是坏蛙。 我们从这些结果为TT的配对中，各拿一个组成新的子集(也就是大小为m的结果) 但我们这里需要证明 新子集中T比F多 配对的实际情况（不是结果）无非是TT，TF，FT，FF。我们现在无非需证明，哪怕在最坏的情况下，也就是所有FF都互相包庇判断为TT，其组数也不可能超过实际TT的组数。 由于无论是FT还是TF，去掉它们对于相对大小的判断都不会有影响，因此TT与FF组数的相对多少，恰好符合原集合中T与F的相对多少。因此TT组数必然小于FF组数。 c) 如果n是奇数怎么办？且最多只能互评 $\lfloor n/2 \rfloor​$次 很简单，直接把多出来的那一个拿进来就好了，但这里问题又回到了证明子集中T的组数需要 $\lceil m \rceil​$ 很显然，当推至”最坏的情况“时，我们需要多出来的那一个也为好蛙，那么为什么呢？ 因为在奇数的情况下，T的总数本身就是比F至少大1的，而FT和TF不影响相对多少，因此我们只用看FF全部说谎的极端情况，但是哪怕如此，T的总数大于F，虽然TT组数极端情况下和FF组数相等，但还剩一个啊！ 所以那个肯定是好蛙！ d) 假设这个函数是downsize(A)，A为蛙全体。 那么就运行这个函数直到A的size为1即可。最后这只留下的一定是一直好蛙。 e) Inductive Hypothesis: 在第$k​$次downsize后，$m_k\le\frac{n}{2^k} +1​$，且子集满足好蛙严格过半 Base Case: $k=0$ ，按照原题意，好蛙显然过半 Indective Step: 由downsize函数在n为奇数与偶数两种情况下的讨论，充其量m在全程均为奇数。 $${2k}+\frac{1}{2k}+…+\frac{1}{2}&lt;\frac{n}{2^k}+1$$ Conclusion: 当达到 $m=1​$ 时，由于size为奇数时严格过半，好蛙的个数至少为 $\lceil1/2\rceil=1​$ ，必为好蛙 f）每一次downsize的效率为 $O(m_k)$, 而 $m_k\le\frac{n}{2_k}+1$， $\therefore O(n)+O(m_1)+O(m_2)…+O(1)=O(n)$ g）你找到一个好蛙…让它在判断一遍所有蛙不就好了？反正它永远说真话啊！ 最后一次扫描，也就 $O(n)$ ，加上之前的 $O(n)$ ，还是 $O(n)$ ​]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# Devide-and-conquer, Mergesort, and Big-O notation]]></title>
    <url>%2F2019%2F02%2F06%2FCS161_L2%2F</url>
    <content type="text"><![CDATA[对应CS161的Lecture2 Correctness and Running Time 算法中最关键的是两个问题 Correctness //Does it work? Running Time //How fast? 今天要讲的正是这两个问题，引入loop invariant这个概念 //这个概念非常重要，是证明算法正确性的利器 整体而言，我们依次会讲解以下两个算法 InsertionSort MergeSort 我们会从一开始提到的两个角度对这两个算法进行分析 InsertionSort 注意这里是python代码，range(l,r)和A[l:r]均是前闭后开 流程非常简单，每一次取已排部分末尾，像夹娃娃机一样，用current夹起来，其他元素让出位子，然后用current放上去，形成新的有序序列。 Correctness 事实上从过程的语言叙述中，我们就可以看出这个算法的一部分正确性证明了。 但是显然这样证明不严谨，这里我们引入loop invariant （下称 LI ） 我们这里摘录一下Wikipedia中的部分解释 LI is a property of a program loop that is true before (and after) each iteration. Knowing its invariant(s) is essential in understanding the effect of a loop. 证明整个过程需要分四步，我们会在下面演示 (注意以下步骤中A下标从0开始，而第 $i=n$ 次循环的意义也取决于$i​$的循环初始值) Inductive Hypothesis: 在第$i=n$次循环之后，$A[:n+1]$有序 Base case: 第$i=0$循环后(也就是$i=1$开始前)，$A[:1]$只有1个，显然有序 Inductive step: 假如现在第$i-1$次循环结束，$A[:i]$有序；那么在经过第$i$次循环的插入操作后（内部$j$循环），$A[:i+1]$如下 $$ A[0],A[1],\dots,A[j^],\dots,A[i-1],A[i] $$ 其中$A[j^]$ 以左均为小于它的元素，以右均为大于它的元素，且左右各自相对有序，则显然现在经过了第$i$次循环，$A[:i+1]$整体有序 Conclusion: 当循环终止时，也就是第$len(A)-1$次循环后，$A[:len(A)]$有序，目标达成 至此我们的算法证明完毕。 在这里我们可以获得的一点感悟是，读懂循环的所谓目的，其实流程无非是找到Inductive Hypothesis，并且让Conclusion能够为我们的实现目标。 Running Time 这里我们不难看到，若序列长为$n$，外层循环$n$次，而内层$j$最多循环$i$次，那么精确一些的结果是 $$ 1+2+\dots+n=\frac{n(n+1)}{2} $$ 但是实际上我们并不care这个常数，因此其实是直接当做$n^2$来看。至于原因，后面会提。 MergeSort 绿色处为哨兵！ Correctness 这里需要先证明一个recursion invariant 不难看出算法分析中非常喜欢invariant，也就是被操作对象维持一定的property，这个property最后可以帮助我们证明算法的正确性。//比如在这里，对象就是无序序列，目标就是让它变有序。 也就是Merge以后的结果必须要有序，所以最终证明变成了证明 Merge可以正确执行它的工作。 所以接下来问题又回到了loop invariant，继续开始我们的四步曲吧！ Inductive Hypothesis: (1)在第$k$次循环后，$A[p:(k+1)]$ 有序 (2)$i,j$ 指向对应组中最小的元素。 Base case: 当$k=p-1$时，$A[p:p]$ 为空集，(1)显然成立。i=j=1,显然也符合(2) Inductive step: 假如现在第$k-1$ 次循环后，$A[p:k]$ 有序,且$L[i]$ 和$R[j]$ 最小。则经过第$k$ 次循环后，$L[i]$ 与$R[j]$ 取较大值加入$A[k]$，不妨假设现在取得是$L[i]$，那么经过$i+=1$ 之后，显然$L[i]$ 仍是L剩下中最小，R中情况不变，因此显然(2)成立；而$A[:k+1]$ 在$A[:k]$ 有序的基础上，满足$A[:k]$ 所有元素均小于$A[k]$，且$A[:k]$ 内部有序，$A[k+1]$ 显然有序。 Conclusion: 当$k=r$循环完成时,$A[p:(r+1)]$ 有序，达成目标。 Running Time 证明分支算法的时间效率，一般利用递推式，和画出以上的recursion tree两种 似乎实质是一样的，只不过画出树来稍微直观一些（虽然递推明明也很直观啊） 首先我们分析一下Merge函数的效率，很容易得到为$O(n)$ //具体分析过程只要数操作数就行了，因为MERGE函数并没有内部调用只有顺序执行 分析完Merge函数，我们要开始分析整个MergeSort $$ \begin{align*} T(n) &amp;= T(n/2)+T(n/2)+O(n)\ &amp;=2\cdot T(n/2)+O(n) \end{align*} $$ 由上面画出的这颗树也很容易看到，对于第t层而言，任务有$2t$个，每个规模为$\frac{n}{2t}​$ 那么对于单层而言，他们“自己做的活”(不包括交给子任务的) $$ \begin{align*} Work\ at\ Level\ i &amp;= (number\ of\ subproblems) \cdot (work\ per\ subproblem)\ &amp;= 2^i \cdot O(\frac{n}{2^i})\ &amp;=O(n) \end{align*} $$ 而层数共有$\log_2{n}$层，于是很容易得到 $$ \begin{align*} Total\ Work &amp;= (Work\ per\ level)\cdot(number\ of\ levels) \ &amp;=O(n)\cdot \log_2(n)\ &amp;=O(n\log_2{n}) \end{align*} $$ 于是我们得到了最终答案。 实际上对于这类子问题规模相同的算法，我们可以用master method进行计算。 在这里实际上已经有了一些master method的影子，需要好好品味。 Guiding Principle 对于算法的运行快慢的评估，我们究竟应该用什么指标去评价？这里会给出答案 Worst-Case Analysis 既然是算法，自然要考虑最坏的情况，毕竟做好最坏的打算肯定是没错的（严肃脸） 我们可以想象我们在进行一场对垒，对手希望用最恶心的数据打败我们 //让我们的算法跑的更慢 那么我们需要保证的是，哪怕他给出最恶心的数据，我也能hold住 好吧说白了就是ACM里的抗hack… Asymptotic Analysis 我们另一个关心的指标是渐近效率。用人话讲，就是当$n$非常大的时候，它会以怎样的趋势增长。 这实际上正是我们忽略常数的原因，因为我们当$n$很大时它的增长趋势并不会怎么受常数影响。 当然！这不代表我们认为常数真的不重要，我们之所以在此忽略常数是因为 当我们用理论流程和伪代码去表示算法时，讨论常数其实是没有意义的。因为你压根不知道常数是多少！这取决于你的机器和使用的语言等。 Asymtotic Notation 事实上渐近是一个数学概念，只是算法里用到比较多而已。 既然如此，我们不妨用数学符号定义一下这个概念。 “Big-Oh” Notation $$ T(n)=O(f(n)) \Leftrightarrow \exists c,n_0&gt;0.\ \forall n \ge n_0, 0\le T(n) \le c\cdot f(n) $$ 从表达式中不难看出，这个$f(n)$代表渐近上界 “Big-Omega” Notation $$ T(n)=\Omega(f(n)) \Leftrightarrow \exists c,n_0&gt;0.\ \forall n \ge n_0, 0\le c\cdot f(n) \le T(n) $$ 类似地，这个$f(n)​$代表渐近下界 “Big-Theta” Notation $$ T(n)=\Theta(f(n)) \Leftrightarrow \exists c_1,c_2,n_0&gt;0.\ \forall n \ge n_0, 0\le c_1f(n) \le T(n) \le c_2 f(n) $$ 当$O(f(n))=\Omega(f(n))​$时，则称$T(n)=\Theta(f(n))​$ 当然，上面给出的这个表达其实也是等价的。 最后给出一个比较直观的图，描述了 $\Theta,\ \Omega,\ O​$ 的关系 Algorithm Implementation 这里我们再顺便把几种sort算法用python实现一下 这里尤其要注意，MergeSort中的哨兵，对于边界条件的控制作用（卡死不让访问无效内存） BubbleSort InsertionSort SelectionSort MergeSort 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566INF = 0xffffffffffffffdef BubbleSort(A): for i in range(len(A)-1): for j in range(len(A)-1, i, -1): if(A[j]&lt;A[j-1]): A[j], A[j-1] = A[j-1], A[j] return Adef InsertionSort(A): for i in range(1, len(A)): current = A[i] j = i-1 while (j&gt;=0 and A[j]&gt;=current): A[j+1] = A[j] j -= 1 A[j+1] = current return Adef SelectionSort(A): for i in range(len(A)-1): minx = INF minPos = -1 for j in range(i, len(A)): if(A[j] &lt; minx): minx = A[j] minPos = j A[i], A[minPos] = A[minPos], A[i] return Adef Merge(L,R): m = len(L) + len(R) #哨兵！ L += [INF] R += [INF] S = [] i = 0 j = 0 for k in range(m): if(L[i]&gt;R[j]): S += [R[j]] j += 1 else: S += [L[i]] i += 1 return Sdef MergeSort(A): r = len(A)-1; if (0 &gt;= r): return A mid = (0+r)//2 L = MergeSort(A[:(mid+1)]) R = MergeSort(A[(mid+1):]) return Merge(L, R)if __name__=="__main__": arr = [4,1,3,9,10,7,2,0] print("Origin Arr\t",arr) print("MergeSort\t",MergeSort(arr)) ans1 = [4,1,3,9,10,7,2,0] print("BubbleSort\t",BubbleSort(arr)) arr = [4,1,3,9,10,7,2,0] print("InsertionSort\t",InsertionSort(arr)) arr = [4,1,3,9,10,7,2,0] print("SelectionSort\t",SelectionSort(arr))]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Algorithm Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# Do you know how to multiply integers?]]></title>
    <url>%2F2019%2F02%2F05%2FCS161_L1%2F</url>
    <content type="text"><![CDATA[对应CS161的Lecture 1 Method to multiply integers 我天真的以为L1不会有什么东西的，看了一眼居然出了overview还真有东西，特此补上。 这一节大概讲了那么两个东西 Graduate-School Integer Multiplication Karatsuba Integer Multiplication //别问我这名字为什么这么鬼畜😳 Graduate-School Integer Multiplication 不分析了…就是从小做到大的竖式进位乘法 真的有什么要说的话，这节课里提到了一个看上去很Tricky的方法 把数字各自分块成两块: $$ x=10{n/2}+b, y=10{n/2}+d\ x*y =10nac+10{n/2}(ad+bc)+bd $$ 但是事实上，在这里如果我们分别对$ac,\ ad, \ bc, \ bd$进行计算，那么实际效率并没有提升 $$ T(n)=4T(n/2)+O(n) $$ 在后面利用Master Method进行分析的结果，容易得到$T(n)=O(n^2)$ Karatsuba Integer Multiplication 事实上，在这里虽然$x*y$已经是一个&quot;Information efficient&quot;的表示，但并不意味着它&quot;Computation efficient&quot; 既然乘法对于我们而言是一个代价较大的操作，那么我们能不能尽量考虑减少乘法。（哪怕增多加减法） $$ ad+bc=(a+b)(c+d)-ac-bd $$ 似乎有一些神奇的事情发生了，原本两次乘法的操作似乎现在用一次就可以完成了呢！ 于是原本 $T(n)$，也就是Worst Running Time，的递推式似乎也发生了变化！ $$ T(n)=3T(n/2)+O(n) $$ 我们再次利用Master Method，得到的新的结果$T(n)=O(n^{\log_2{3}})\approx O(n^{1.6})$ 似乎的确是个略优的算法呢！ 然后我动手实现了一下这个诡异的算法 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom multHelpers import *#makeInt(x)#getDigits(X)#def multABunch(myFn, nVals, numTrials=20#myFn - Function to be used in multiplication#nVals - List include different length for data generator#numTrials - how many times of testing for each length#x - List, Y - Intdef karatsubaMult(X, Y): x = getDigits(X) y = getDigits(Y) nx = len(x) ny = len(y) n = max(nx, ny) x = [0 for i in range(n-nx)] + x y = [0 for i in range(n-ny)] + y return karatsubaMult_Helper(x, y)def karatsubaMult_Helper(x, y): n = len(x) if (n!=1 and n%2==1): x = [0] + x y = [0] + y n = len(x) if(n==1): return x[0]*y[0] a = x[:n//2] b = x[n//2:] c = y[:n//2] d = y[n//2:] termAC = karatsubaMult_Helper(a, c) termBD = karatsubaMult_Helper(b, d) termMID = karatsubaMult(makeInt(a)+makeInt(b), makeInt(c)+makeInt(d))-termAC-termBD ret = (10**n)*termAC+(10**(n//2))*termMID+termBD #print(x, y, termAC, termBD, termMID, ret) return retif __name__=="__main__": X = 212342131398 Y = 25415243554767 print(X*Y) print(karatsubaMult(X, Y))]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Karatsuba Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《认知天性》小感]]></title>
    <url>%2F2019%2F02%2F05%2F%E8%AE%A4%E7%9F%A5%E5%A4%A9%E6%80%A7%E5%B0%8F%E6%84%9F%2F</url>
    <content type="text"><![CDATA[还没更完，有关“穿插学习”的内容仍需跟进 这本书其实之前看了以后一直想讲一讲，真的纠正了自己之前很多错误的观念。 认知天性这样的翻译似乎有点莫名其妙，但是这里看一下它的英文书名 "MAKE IT STICK: THE SCIENCE OF SUCCESSFUL LEARNING" 正如它的书名一样，学习的关键在于STICK，但这里又不能简单理解为“记住”，而是在深刻记忆的基础上，对知识进行化用，直到让知识嵌入自己的心智模型。 之前我分享读后感或者读书笔记的方式，只是沿用了《如何阅读一本书》中的观点，也就是 先掌握整本书的叙述框架(目录+粗读)，再在基本框架的基础上进行填充(精读)。 但在具体的执行上，似乎显得有些“目录主义”，再加上自己有些强迫症，于是本来的读书笔记成为了“抄书大会”。 于是最后的结果是，我以为写完笔记自己就真的懂了。 停止对你的课本“浓妆艳抹” 然后今天的这本书告诉我，全tm是错觉。 现在的学生喜欢在书上画满各种花花绿绿的线，这里画个框，那里圈个圈。以为自己抓住了重点。 但事实是，你以为自己抓住了重点。但是你的大脑也罢工了。 你的大脑告诉自己，这个东西突出出来，我的任务已经完成了，我不需要记住它了。 这其实跟现在的资料搜集狂是有一些相似的，收藏夹里都快爆炸了，但是真的自己吸收过得干货根本没有多少。 为什么呢？因为你把它扔进收藏夹里的那一刻，你就是在告诉自己 “反正我以后会去看它的。” 事实上你没有…你还是在不断搜集资料的路上，但是那些好的经验与方法，你一个也没吸收。 这就是为什么韩寒说 我们听过很多道理，却依然过不好这一生。 废话！你听过很多道理，却一个都没记住，一个也没实践，能过好就怪了。 很多人觉得听到某句话时，真的是触动灵魂！决定把这句话当做自己的motto。 然后第二天起床，什么？昨天我干了什么呢。 人就是这么废掉的。 学习需要“刻意” 究其原因，是因为你太习惯舒适了，而学习本就是痛苦的。 人天生易忘，但偏偏学习的基础就是记忆。而有些人就是觉得做完笔记把书扔一边，自己就能学会所有的东西。 当然，有的人很勤奋，他们一遍又一遍的读书，觉得读的越来越顺对书越来越熟，就是掌握了知识。 但是合上书呢? 这里樊登老师给出了一个很形象的比喻：捋绳子。 知识就像绳子，你的每一次不厌其烦地看书，就像不断地去捋绳子 结果每次都是从头捋到尾，脑子里啥都没留下 但是记忆是要给这绳子打结的！ 那到底怎么要才能记得住？考试！ 用检测帮助记忆 可能大家一听到考试就讨厌，那是因为考试往往和成绩挂钩。而成绩…太被看重了。 但是这里需要清楚的一点是，考试本身是有助于学习的。我们看一下考试的一种理解 在【没有参考资料的情况下独立完成】的【知识检索】 核心，其实简洁一点说，就是**“强迫检索”** 强迫，避免了自欺欺人，也就是在你明明都还没想起来的情况下看到课本，然后当个“秒懂”的“事后诸葛亮”。 而检索，才是学习的核心。当然，这里的检测是需要有反馈的，不然你学到的东西是对是错都无法确定，用检测去纠正自己的误解，也是检测很必要的一面。 但是正如我前面提到的，大家都讨厌考试，更讨厌在考试里犯错，因为犯错往往意味着考砸。所以有些人，会在自我检测的时候，当一个对自己极为宽松的“良师”，模棱两可都算对。那么真的在考试的时候，你是指望每个老师都这样对待你这份模棱两可的试卷吗？ 这里其实在《刻意练习》和《成长型思维》两本书中都有提到 重在过程 越盯着结果不放，反而越是无力进步。 observe-&gt;practice-&gt;progress 这才是正确的学习之道 从检索说开去 从个人的理解而言，其实检索和之前自己读到过的很多东西都是有关系的。 比如《如何阅读一本书》中提到的积极阅读，以及在英语学习中的“表达越用越熟”的说法，其实都是有关系的。从通俗一点的角度来讲，我觉得可以概括为一句话 脑子越用越活 这某种程度上也是我想着开始写博客的目的。 之前听说过有人会在英语的学习中选择一种输出倒逼输入的方式，也就是强行要你写东西，因为写不出来，就会想着去查各种表达，最后完成一篇文章。 这种逆向学习的方法在“知识搜集”上显得有一些功利，而且可能学到的东西显得有点碎片化。毕竟你一次也就用一个表达，比如 坚持立场 = dig one’s heels in，然后在你脑子里，这个表达就只是一个孤立的点，也就很难让你想到它。就像一座孤岛，你在海上找，很难找到。 如果说孤岛很难寻找，那么群岛呢？ 你查了dig one’s heels in，为什么不查查stand up for， 为什么不查查stand one’s ground。maintain the position 和 fight one’s corner和其他的有什么区别？如果区别不大，能不能一起用？ 查了 坚持立场，你问问自己，你能想到坚持常常和什么搭配吗？firm, dogged, strong-willed, tough, persistent? 它们的theraurus里的rigid, stubborn, stern, harsh？似乎和坚持有些不一样。但总归有些共同点吧？ 于是每一次查字典，一片一片地查，你每一次获得的都是一个群岛。 好处是什么呢？你找到群岛的一角，那么离你想要的那个孤岛，也就不远了。 学习不就是这样吗？主动建立连接，你去找知识，别等着知识去找你。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>高效学习</tag>
      </tags>
  </entry>
</search>
