<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[#CMU15-213# Everything We Need]]></title>
    <url>%2F2019%2F02%2F09%2FCMU15-213_L0%2F</url>
    <content type="text"><![CDATA[Everything We Need 使用的教材为CSAPP 3th 站点资料 包括课程资料和博客参考 课程首页 这里选用2018 fall的课程主页 主要是课件比较齐全，包括习题课(recitation)讲义和每节课的lecture 博客参考 这是一个在CMU读master的大神的博客 我真的难以想象他是怎么做到看书这么快还能写博文的...]]></content>
      <categories>
        <category>计算机原理</category>
      </categories>
      <tags>
        <tag>CMU15-213</tag>
        <tag>Data Integration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# ProblemSet1 Solution]]></title>
    <url>%2F2019%2F02%2F07%2FCS161_P1%2F</url>
    <content type="text"><![CDATA[PoblemSet1 Finished Solution 这里完成以下ProblemSet 1 对应章节大概是L1, L2 Exercise 看完了... 取\(n_0=5, c=8\),满足\(n&gt;n_0\)时，\(2\sqrt{n}+6&lt;8\sqrt{n}\) A）显然是 \(O(n)​\)。for循环总共n次，每次进行判断（以及一次赋值），但常数不重要，所以1op和2op不区分 B）取 \(n_0=20000, c=0.0001​\)。 原lst长度太小了，无法体现渐近，调整到了万级 C） 通过绘图的最远两个点大概估计了一下斜率。据此可算出\(n=10^{15}\)时大概需要花的时间 a）\(T(n)=2\sqrt{n}+6,\ g(n)=\sqrt{n}​\) 取 \(n_0=2, c=8​\)，显然满足\(\forall n&gt;2，2\sqrt{n}+6&lt;8\sqrt{n}​\) \(\therefore 2\sqrt{n}+6=O(\sqrt n)​\) b）\(T(n)=n^2, \ g(n)=n\) 取 \(n_0=2, c=1\), 显然满足 \(\forall n&gt;2, 2n^2&gt;n\) \(\therefore n^2=\Omega(n)\) c）\(T(n)=\log_2{n},\ g(n)=\ln(n)\) 取 \(n_0=1, c_1=\log_2(e-1),c_2=\log_2(e+1)\) ， 显然满足\(\forall n&gt;1,\ \log_2(e-1)\ln{n}&lt;log_2n&lt;\log_2(e+1)\ln n\) \(\therefore \log_2(n) = \Theta(\ln n)\) d）假设 \(4^n=O(2^n)​\)，那么应该有 \(\forall n &gt;n_0, 4^n&lt;c2^n​\)，也就是 \(c&gt;2^n​\)。 但是对于任一\(n_0​\) ，满足这个条件的常数\(c​\)都不存在 \(\therefore 4^n\neq O(2^n)​\) Problems (a)和(b)逻辑是没问题的，但是(c)是错的。(c) 最致命的一点是把\(O(1)\) 次”操作“和\(O(1)\) 的时间效率混淆。那按这么说，推到极端，所有程序都是\(O(1)\) 的，因为他们只调用了一次main。 题意简单而言就是有n只青蛙，有好蛙和坏蛙，让他们相互评价，一次评价叫一次”toad-to-toad comparison“(此处因政治因素不对toad做直译，下称互评)。最终要求通过 \(O(n)\) 次互评判断所有好坏。 a）\(O(n^2)​\)的算法相当简单，对于任意一青蛙，让其与其他n-1只青蛙进行互评即可。 假设你是坏蛙，那么剩下至少一半的好蛙一定会说你是坏蛙。 假设你是好蛙，那么剩下至少一半的好蛙一定会说你是好蛙。 结果就是，”大多数蛙的判断是对的。“ b）这里要求的函数功能是： 通过至多n/2次互评找出一个大小为m(m至多为\(n/2\))的子集 这个子集至少有一半是好蛙。 给我们n/2次操作，那不难相当分成两组对应互评这一策略，问题是怎样通过结果找到这个子集呢？ 我们可以建立一个逆向真值表，也就是通过互评结果看看有哪几种可能 互评结果 实际可能 TT TT or FF TF FT or FF FT TF or FF FF TF or FT or FF 可以看出，只有在结果为TT时，我们从{A, B}中取出的A与B的关系是确定的，要么都是好蛙，要么都是坏蛙。 我们从这些结果为TT的配对中，各拿一个组成新的子集(也就是大小为m的结果) 但我们这里需要证明 新子集中T比F多 配对的实际情况（不是结果）无非是TT，TF，FT，FF。我们现在无非需证明，哪怕在最坏的情况下，也就是所有FF都互相包庇判断为TT，其组数也不可能超过实际TT的组数。 由于无论是FT还是TF，去掉它们对于相对大小的判断都不会有影响，因此TT与FF组数的相对多少，恰好符合原集合中T与F的相对多少。因此TT组数必然小于FF组数。 c）如果n是奇数怎么办？且最多只能互评\(\lfloor n/2 \rfloor​\)次 很简单，直接把多出来的那一个拿进来就好了，但这里问题又回到了证明子集中T的组数需要 \(\lceil m \rceil​\) 很显然，当推至”最坏的情况“时，我们需要多出来的那一个也为好蛙，那么为什么呢？ 因为在奇数的情况下，T的总数本身就是比F至少大1的，而FT和TF不影响相对多少，因此我们只用看FF全部说谎的极端情况，但是哪怕如此，T的总数大于F，虽然TT组数极端情况下和FF组数相等，但还剩一个啊！ 所以那个肯定是好蛙！ d） 假设这个函数是downsize(A)，A为蛙全体。 那么就运行这个函数直到A的size为1即可。最后这只留下的一定是一直好蛙。 e）Inductive Hypothesis: 在第\(k​\)次downsize后，\(m_k\le\frac{n}{2^k} +1​\)，且子集满足好蛙严格过半 Base Case: \(k=0\) ，按照原题意，好蛙显然过半 Indective Step: 由downsize函数在n为奇数与偶数两种情况下的讨论，充其量m在全程均为奇数。 \[ {2^k}+\frac{1}{2^k}+...+\frac{1}{2}&lt;\frac{n}{2^k}+1 \] Conclusion: 当达到 \(m=1​\) 时，由于size为奇数时严格过半，好蛙的个数至少为\(\lceil1/2\rceil=1​\) ，必为好蛙 f）每一次downsize的效率为 \(O(m_k)\), 而 \(m_k\le\frac{n}{2_k}+1\)， \(\therefore O(n)+O(m_1)+O(m_2)...+O(1)=O(n)\)。 g）你找到一个好蛙...让它在判断一遍所有蛙不就好了？反正它永远说真话啊！ 最后一次扫描，也就 \(O(n)\) ，加上之前的 \(O(n)\)，还是 \(O(n)\) ​]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# Devide-and-conquer, Mergesort, and Big-O notation]]></title>
    <url>%2F2019%2F02%2F06%2FCS161_L2%2F</url>
    <content type="text"><![CDATA[对应CS161的Lecture2 Correctness and Running Time 算法中最关键的是两个问题 Correctness //Does it work? Running Time //How fast? 今天要讲的正是这两个问题，引入loop invariant这个概念 //这个概念非常重要，是证明算法正确性的利器 整体而言，我们依次会讲解以下两个算法 InsertionSort MergeSort 我们会从一开始提到的两个角度对这两个算法进行分析 InsertionSort 注意这里是python代码，range(l,r)和A[l:r]均是前闭后开 流程非常简单，每一次取已排部分末尾，向夹娃娃机一样，用current夹起来，其他元素让出位子，然后用current放上去，形成新的有序序列。 Correctness 事实上从过程的语言叙述中，我们就可以看出这个算法的一部分正确性证明了。 但是显然这样证明不严谨，这里我们引入loop invariant （下称 LI ） 我们这里摘录一下Wikipedia中的部分解释 LI is a property of a program loop that is true before (and after) each iteration. Knowing its invariant(s) is essential in understanding the effect of a loop. 证明整个过程需要分四步，我们会在下面演示 (注意以下步骤中A下标从0开始，而第 \(i=n\) 次循环的意义也取决于\(i​\)的循环初始值) Inductive Hypothesis: 在第\(i=n\)次循环之后，\(A[:n+1]\)有序 Base case: 第\(i=0\)循环后(也就是\(i=1\)开始前)，\(A[:1]\)只有1个，显然有序 Inductive step: 假如现在第\(i-1\)次循环结束，\(A[:i]\)有序；那么在经过第\(i\)次循环的插入操作后（内部\(j\)循环），\(A[:i+1]\)如下 \[ A[0],A[1],\dots,A[j^*],\dots,A[i-1],A[i] \] 其中\(A[j^*]\) 以左均为小于它的元素，以右均为大于它的元素，且左右各自相对有序，则显然现在经过了第\(i\)次循环，\(A[:i+1]\)整体有序 Conclusion: 当循环终止时，也就是第\(len(A)-1\)次循环后，\(A[:len(A)]\)有序，目标达成 至此我们的算法证明完毕。 在这里我们可以获得的一点感悟是，读懂循环的所谓目的，其实流程无非是找到Inductive Hypothesis，并且让Conclusion能够为我们的实现目标。 Running Time 这里我们不难看到，若序列长为\(n\)，外层循环\(n\)次，而内层\(j\)最多循环\(i\)次，那么精确一些的结果是 \[ 1+2+\dots+n=\frac{n(n+1)}{2} \] 但是实际上我们并不care这个常数，因此其实是直接当做\(n^2\)来看。至于原因，后面会提。 MergeSort 绿色处为哨兵！ Correctness 这里需要先证明一个recursion invariant 不难看出算法分析中非常喜欢invariant，也就是被操作对象维持一定的property，这个property最后可以帮助我们证明算法的正确性。//比如在这里，对象就是无序序列，目标就是让它变有序。 也就是Merge以后的结果必须要有序，所以最终证明变成了证明 Merge可以正确执行它的工作。 所以接下来问题又回到了loop invariant，继续开始我们的四步曲吧！ Inductive Hypothesis: (1)在第\(k\)次循环后，\(A[p:(k+1)]\)有序 (2)\(i,j\)指向对应组中最小的元素。 Base case: 当\(k=p-1\)时，\(A[p:p]\) 为空集，(1)显然成立。i=j=1,显然也符合(2) Inductive step: 假如现在第\(k-1\) 次循环后，\(A[p:k]\) 有序,且\(L[i]\) 和\(R[j]\) 最小。则经过第\(k\) 次循环后，\(L[i]\) 与\(R[j]\) 取较大值加入\(A[k]\)，不妨假设现在取得是\(L[i]\)，那么经过\(i+=1\) 之后，显然\(L[i]\) 仍是L剩下中最小，R中情况不变，因此显然(2)成立；而\(A[:k+1]\) 在\(A[:k]\) 有序的基础上，满足\(A[:k]\) 所有元素均小于\(A[k]\)，且\(A[:k]\) 内部有序，\(A[k+1]\) 显然有序。 Conclusion: 当\(k=r\)循环完成时, \(A[p:(r+1)]\)有序，达成目标。 Running Time 证明分支算法的时间效率，一般利用递推式，和画出以上的recursion tree两种 似乎实质是一样的，只不过画出树来稍微直观一些（虽然递推明明也很直观啊） 首先我们分析一下Merge函数的效率，很容易得到为\(O(n)\) //具体分析过程只要数操作数就行了，因为MERGE函数并没有内部调用只有顺序执行 分析完Merge函数，我们要开始分析整个MergeSort \[ \begin{align*} T(n) &amp;= T(n/2)+T(n/2)+O(n)\\ &amp;=2\cdot T(n/2)+O(n) \end{align*} \] 由上面画出的这颗树也很容易看到，对于第t层而言，任务有\(2^t\)个，每个规模为\(\frac{n}{2^t}​\) 那么对于单层而言，他们“自己做的活”(不包括交给子任务的) \[ \begin{align*} Work\ at\ Level\ i &amp;= (number\ of\ subproblems) \cdot (work\ per\ subproblem)\\ &amp;= 2^i \cdot O(\frac{n}{2^i})\\ &amp;=O(n) \end{align*} \] 而层数共有\(\log_2{n}\)层，于是很容易得到 \[ \begin{align*} Total\ Work &amp;= (Work\ per\ level)\cdot(number\ of\ levels) \\ &amp;=O(n)\cdot \log_2(n)\\ &amp;=O(n\log_2{n}) \end{align*} \] 于是我们得到了最终答案。 实际上对于这类子问题规模相同的算法，我们可以用master method进行计算。 在这里实际上已经有了一些master method的影子，需要好好品味。 Guiding Principle 对于算法的运行快慢的评估，我们究竟应该用什么指标去评价？这里会给出答案 Worst-Case Analysis 既然是算法，自然要考虑最坏的情况，毕竟做好最坏的打算肯定是没错的（严肃脸） 我们可以想象我们在进行一场对垒，对手希望用最恶心的数据打败我们 //让我们的算法跑的更慢 那么我们需要保证的是，哪怕他给出最恶心的数据，我也能hold住 好吧说白了就是ACM里的抗hack... Asymptotic Analysis 我们另一个关心的指标是渐近效率。用人话讲，就是当\(n\)非常大的时候，它会以怎样的趋势增长。 这实际上正是我们忽略常数的原因，因为我们当\(n\)很大时它的增长趋势并不会怎么受常数影响。 当然！这不代表我们认为常数真的不重要，我们之所以在此忽略常数是因为 当我们用理论流程和伪代码去表示算法时，讨论常数其实是没有意义的。因为你压根不知道常数是多少！这取决于你的机器和使用的语言等。 Asymtotic Notation 事实上渐近是一个数学概念，只是算法里用到比较多而已。 既然如此，我们不妨用数学符号定义一下这个概念。 - &quot;Big-Oh&quot; Notation \[ T(n)=O(f(n)) \Leftrightarrow \exist c,n_0&gt;0.\ \forall n \ge n_0, 0\le T(n) \le c\cdot f(n) \] 从表达式中不难看出，这个\(f(n)\)代表==渐近上界== - &quot;Big-Omega&quot; Notation \[ T(n)=\Omega(f(n)) \Leftrightarrow \exist c,n_0&gt;0.\ \forall n \ge n_0, 0\le c\cdot f(n) \le T(n) \] 类似地，这个\(f(n)\)代表==渐近下界== - &quot;Big-Theta&quot; Notation \[ T(n)=\Theta(f(n)) \Leftrightarrow \exist c_1,c_2,n_0&gt;0.\ \forall n \ge n_0, 0\le c_1f(n) \le T(n) \le c_2 f(n) \] 当\(O(f(n))=\Omega(f(n))\)时，则称\(T(n)=\Theta(f(n))\) 当然，上面给出的这个表达其实也是等价的。 最后给出一个比较直观的图，描述了\(\Theta,\ \Omega,\ O\)的关系 Algorithm Implementation 这里我们再顺便把几种sort算法用python实现一下 这里尤其要注意，MergeSort中的哨兵，对于边界条件的控制作用（卡死不让访问无效内存） BubbleSort InsertionSort SelectionSort MergeSort 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566INF = 0xffffffffffffffdef BubbleSort(A): for i in range(len(A)-1): for j in range(len(A)-1, i, -1): if(A[j]&lt;A[j-1]): A[j], A[j-1] = A[j-1], A[j] return Adef InsertionSort(A): for i in range(1, len(A)): current = A[i] j = i-1 while (j&gt;=0 and A[j]&gt;=current): A[j+1] = A[j] j -= 1 A[j+1] = current return Adef SelectionSort(A): for i in range(len(A)-1): minx = INF minPos = -1 for j in range(i, len(A)): if(A[j] &lt; minx): minx = A[j] minPos = j A[i], A[minPos] = A[minPos], A[i] return Adef Merge(L,R): m = len(L) + len(R) #哨兵！ L += [INF] R += [INF] S = [] i = 0 j = 0 for k in range(m): if(L[i]&gt;R[j]): S += [R[j]] j += 1 else: S += [L[i]] i += 1 return Sdef MergeSort(A): r = len(A)-1; if (0 &gt;= r): return A mid = (0+r)//2 L = MergeSort(A[:(mid+1)]) R = MergeSort(A[(mid+1):]) return Merge(L, R)if __name__=="__main__": arr = [4,1,3,9,10,7,2,0] print("Origin Arr\t",arr) print("MergeSort\t",MergeSort(arr)) ans1 = [4,1,3,9,10,7,2,0] print("BubbleSort\t",BubbleSort(arr)) arr = [4,1,3,9,10,7,2,0] print("InsertionSort\t",InsertionSort(arr)) arr = [4,1,3,9,10,7,2,0] print("SelectionSort\t",SelectionSort(arr))]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Algorithm Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# Do you know how to multiply integers?]]></title>
    <url>%2F2019%2F02%2F05%2FCS161_L1%2F</url>
    <content type="text"><![CDATA[对应CS161的Lecture 1 Method to multiply integers 我天真的以为L1不会有什么东西的，看了一眼居然出了overview还真有东西，特此补上。 这一节大概讲了那么两个东西 Graduate-School Integer Multiplication Karatsuba Integer Multiplication //别问我这名字为什么这么鬼畜 Graduate-School Integer Multiplication 不分析了...就是从小做到大的竖式进位乘法 真的有什么要说的话，这节课里提到了一个看上去很Tricky的方法 把数字各自分块成两块: \[ x=10^{n/2}+b,\ y=10^{n/2}+d\\ x*y =10^nac+10^{n/2}(ad+bc)+bd \] 但是事实上，在这里如果我们分别对\(ac,\ ad, \ bc, \ bd\)进行计算，那么实际效率并没有提升 \[ T(n)=4T(n/2)+O(n) \] 在后面利用Master Method进行分析的结果，容易得到\(T(n)=O(n^2)\) Karatsuba Integer Multiplication 事实上，在这里虽然\(x*y\)已经是一个&quot;Information efficient&quot;的表示，但并不意味着它&quot;Computation efficient&quot; 既然乘法对于我们而言是一个代价较大的操作，那么我们能不能尽量考虑减少乘法。（哪怕增多加减法） \[ ad+bc=(a+b)(c+d)-ac-bd \] 似乎有一些神奇的事情发生了，原本两次乘法的操作似乎现在用一次就可以完成了呢！ 于是原本$T(n) $，也就是Worst Running Time，的递推式似乎也发生了变化！ \[ T(n)=3T(n/2)+O(n) \] 我们再次利用Master Method，得到的新的结果\(T(n)=O(n^{\log_2{3}})\approx O(n^{1.6})\) 似乎的确是个略优的算法呢！ 然后我动手实现了一下这个诡异的算法 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom multHelpers import *#makeInt(x)#getDigits(X)#def multABunch(myFn, nVals, numTrials=20#myFn - Function to be used in multiplication#nVals - List include different length for data generator#numTrials - how many times of testing for each length#x - List, Y - Intdef karatsubaMult(X, Y): x = getDigits(X) y = getDigits(Y) nx = len(x) ny = len(y) n = max(nx, ny) x = [0 for i in range(n-nx)] + x y = [0 for i in range(n-ny)] + y return karatsubaMult_Helper(x, y)def karatsubaMult_Helper(x, y): n = len(x) if (n!=1 and n%2==1): x = [0] + x y = [0] + y n = len(x) if(n==1): return x[0]*y[0] a = x[:n//2] b = x[n//2:] c = y[:n//2] d = y[n//2:] termAC = karatsubaMult_Helper(a, c) termBD = karatsubaMult_Helper(b, d) termMID = karatsubaMult(makeInt(a)+makeInt(b), makeInt(c)+makeInt(d))-termAC-termBD ret = (10**n)*termAC+(10**(n//2))*termMID+termBD #print(x, y, termAC, termBD, termMID, ret) return retif __name__=="__main__": X = 212342131398 Y = 25415243554767 print(X*Y) print(karatsubaMult(X, Y))]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Karatsuba Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《认知天性》小感]]></title>
    <url>%2F2019%2F02%2F05%2F%E8%AE%A4%E7%9F%A5%E5%A4%A9%E6%80%A7%E5%B0%8F%E6%84%9F%2F</url>
    <content type="text"><![CDATA[还没更完，有关“穿插学习”的内容仍需跟进 这本书其实之前看了以后一直想讲一讲，真的纠正了自己之前很多错误的观念。 认知天性这样的翻译似乎有点莫名其妙，但是这里看一下它的英文书名 &quot;MAKE IT STICK: THE SCIENCE OF SUCCESSFUL LEARNING&quot; 正如它的书名一样，学习的关键在于STICK，但这里又不能简单理解为“记住”，而是在深刻记忆的基础上，对知识进行化用，直到让知识嵌入自己的心智模型。 之前我分享读后感或者读书笔记的方式，只是沿用了《如何阅读一本书》中的观点，也就是 先掌握整本书的叙述框架(目录+粗读)，再在基本框架的基础上进行填充(精读)。 但在具体的执行上，似乎显得有些“目录主义”，再加上自己有些强迫症，于是本来的读书笔记成为了“抄书大会”。 于是最后的结果是，我以为写完笔记自己就真的懂了。 停止对你的课本“浓妆艳抹” 然后今天的这本书告诉我，全tm是错觉。 现在的学生喜欢在书上画满各种花花绿绿的线，这里画个框，那里圈个圈。以为自己抓住了重点。 但事实是，你以为自己抓住了重点。但是你的大脑也罢工了。 你的大脑告诉自己，这个东西突出出来，我的任务已经完成了，我不需要记住它了。 这其实跟现在的资料搜集狂是有一些相似的，收藏夹里都快爆炸了，但是真的自己吸收过得干货根本没有多少。 为什么呢？因为你把它扔进收藏夹里的那一刻，你就是在告诉自己 “反正我以后会去看它的。” 事实上你没有...你还是在不断搜集资料的路上，但是那些好的经验与方法，你一个也没吸收。 这就是为什么韩寒说 我们听过很多道理，却依然过不好这一生。 废话！你听过很多道理，却一个都没记住，一个也没实践，能过好就怪了。 很多人觉得听到某句话时，真的是触动灵魂！决定把这句话当做自己的motto。 然后第二天起床，什么？昨天我干了什么呢。 人就是这么废掉的。 学习需要“刻意” 究其原因，是因为你太习惯舒适了，而学习本就是痛苦的。 人天生易忘，但偏偏学习的基础就是记忆。而有些人就是觉得做完笔记把书扔一边，自己就能学会所有的东西。 当然，有的人很勤奋，他们一遍又一遍的读书，觉得读的越来越顺对书越来越熟，就是掌握了知识。 但是合上书呢? 这里樊登老师给出了一个很形象的比喻：捋绳子。 知识就像绳子，你的每一次不厌其烦地看书，就像不断地去捋绳子 结果每次都是从头捋到尾，脑子里啥都没留下 但是记忆是要给这绳子打结的！ 那到底怎么要才能记得住？考试！ 用检测帮助记忆 可能大家一听到考试就讨厌，那是因为考试往往和成绩挂钩。而成绩...太被看重了。 但是这里需要清楚的一点是，考试本身是有助于学习的。我们看一下考试的一种理解 在【没有参考资料的情况下独立完成】的【知识检索】 核心，其实简洁一点说，就是“强迫检索” 强迫，避免了自欺欺人，也就是在你明明都还没想起来的情况下看到课本，然后当个“秒懂”的“事后诸葛亮”。 而检索，才是学习的核心。当然，这里的检测是需要有反馈的，不然你学到的东西是对是错都无法确定，用检测去纠正自己的误解，也是检测很必要的一面。 但是正如我前面提到的，大家都讨厌考试，更讨厌在考试里犯错，因为犯错往往意味着考砸。所以有些人，会在自我检测的时候，当一个对自己极为宽松的“良师”，模棱两可都算对。那么真的在考试的时候，你是指望每个老师都这样对待你这份模棱两可的试卷吗？ 这里其实在《刻意练习》和《成长型思维》两本书中都有提到 重在过程 越盯着结果不放，反而越是无力进步。 observe-&gt;practice-&gt;progress 这才是正确的学习之道 从检索说开去 从个人的理解而言，其实检索和之前自己读到过的很多东西都是有关系的。 比如《如何阅读一本书》中提到的积极阅读，以及在英语学习中的“表达越用越熟”的说法，其实都是有关系的。从通俗一点的角度来讲，我觉得可以概括为一句话 脑子越用越活 这某种程度上也是我想着开始写博客的目的。 之前听说过有人会在英语的学习中选择一种输出倒逼输入的方式，也就是强行要你写东西，因为写不出来，就会想着去查各种表达，最后完成一篇文章。 这种逆向学习的方法在“知识搜集”上显得有一些功利，而且可能学到的东西显得有点碎片化。毕竟你一次也就用一个表达，比如 坚持立场 = dig one's heels in，然后在你脑子里，这个表达就只是一个孤立的点，也就很难让你想到它。就像一座孤岛，你在海上找，很难找到。 如果说孤岛很难寻找，那么群岛呢？ 你查了dig one's heels in，为什么不查查stand up for， 为什么不查查stand one's ground。maintain the position 和 fight one's corner和其他的有什么区别？如果区别不大，能不能一起用？ 查了 坚持立场，你问问自己，你能想到坚持常常和什么搭配吗？firm, dogged, strong-willed, tough, persistent? 它们的theraurus里的rigid, stubborn, stern, harsh？似乎和坚持有些不一样。但总归有些共同点吧？ 于是每一次查字典，一片一片地查，你每一次获得的都是一个群岛。 好处是什么呢？你找到群岛的一角，那么离你想要的那个孤岛，也就不远了。 学习不就是这样吗？主动建立连接，你去找知识，别等着知识去找你。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>高效学习</tag>
      </tags>
  </entry>
</search>
