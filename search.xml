<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[#CS161# Analyze T(n) for Recurrence]]></title>
    <url>%2F2019%2F02%2F12%2FCS161_L3L4%2F</url>
    <content type="text"><![CDATA[Analyze T(n) for Recurrence 对应CS161的Lecture 3,4 今天主要讲讲如何分析递推表达的时间效率，也就是如下这种形式 T(n)=f(n)+∑i=0mT(ni)(ni&lt;n)T(n)=f(n)+\sum_{i=0}^mT(n_i)(n_i&lt;n) T(n)=f(n)+i=0∑m​T(ni​)(ni​&lt;n) 我们会着重对分治情况下的分析进行阐述，毕竟这种递推形式肯定是和分治对应的。 我们会引入两种方法，这也是这一讲的重点 Master Method Substitution Method 在展开详细的内容之前，我们需要知道两个方法的限制。 前者只能用于各个 nin_ini​ 相同的情况，而后者更依赖于猜的准不准。 Master Method 首先我们要看一下Master Method解决的问题的形式 T(n)=a⋅T(n/b)+f(n)T(n)=a \cdot T(n/b)+f(n) T(n)=a⋅T(n/b)+f(n) 然后我们来看看Theorem的内容 这里需要注意的是情况1和情况3中的 ϵ\epsilonϵ 的作用，更具体而言，n±ϵn^{\pm \epsilon}n±ϵ 的作用。 这实际上涉及了polynomially larger/less的概念，举个例子而言， O(n) vs O(nlog⁡n)O(n)\ \ vs \ \ O(n\log{n}) O(n) vs O(nlogn) 后者是比前者大但不是polynomially larger，因为 log⁡n\log{n}logn 并不够达到 nϵn^{\epsilon}nϵ 的级别 这是在使用master method的时候非常需要注意的一点。 Example 我们不妨举几个例子，用一用这个方法 T(n)=4⋅T(n/2)+O(n)T(n) = 4 \cdot T(n/2) + O(n)T(n)=4⋅T(n/2)+O(n) 这里 a=4,b=2,f(n)=O(n),log⁡ba=2a=4, b=2, f(n)=O(n), \log_b{a}=2a=4,b=2,f(n)=O(n),logb​a=2 满足case 1 ∴T(n)=Θ(n2)\therefore T(n)=\Theta(n^2)∴T(n)=Θ(n2) T(n)=3⋅T(n/2)+O(n)T(n)=3 \cdot T(n/2) + O(n)T(n)=3⋅T(n/2)+O(n) 这里 a=3,b=2,f(n)=O(n),log⁡ba=log⁡23a=3, b=2, f(n)=O(n), \log_b{a}=\log_2{3}a=3,b=2,f(n)=O(n),logb​a=log2​3 满足case 1 ∴T(n)=Θ(nlog⁡23)\therefore T(n)=\Theta(n^{\log_2{3}})∴T(n)=Θ(nlog2​3) T(n)=2⋅T(n/2)+O(n)T(n)=2 \cdot T(n/2) + O(n)T(n)=2⋅T(n/2)+O(n) 这里 a=2,b=2,f(n)=O(n),log⁡ba=log⁡22a=2, b=2, f(n)=O(n), \log_b{a}=\log_2{2}a=2,b=2,f(n)=O(n),logb​a=log2​2 满足case 2 ∴T(n)=Θ(n⋅log⁡n)\therefore T(n)=\Theta(n \cdot \log{n})∴T(n)=Θ(n⋅logn) 由于证明有点冗长，在这里对主定理先不做证明 Substitution method 相比主定理这种并不是那么直觉的表述，代入法就显得很直觉了 法如其名，我们只要猜一猜，带进去。如果能够自洽，就是我们需要的结果。 k-Selection Problems 在考虑应用代入法前，我们先看一个问题 对于一个数组，如何求出其中第k大的值 最暴力的想法显然是直接排序了，但是我们要想想有没有其他可能，比如分治？ 假如我们每次把答案可能存在的区间不断缩小，有没有可能做到。 比如取一pivot，根据pivot将数组重排两段，左边都小于pivot，右边都大于pivot 我们是不是可以很容易判断，第k大应该在哪一段，然后扔掉另一端呢？ 那么关键在于，pivot怎么取，因为pivot的选取直接影响了 T(n)T(n)T(n) 假设每次都取最大或最小值，这显然是个糟糕的决定。 T(n)=T(n−1)+O(n)T(n)=T(n-1)+O(n) T(n)=T(n−1)+O(n) 显然会退化成一个 O(n2)O(n^2)O(n2) 的算法 但是如果我们每次取中位数又不可能，因为我们的问题本来就是找第k大！ 那么我们退而求其次，我们求一下待在比较中间的数 说白了，就是五个元素一组，分别排序取中位数，然后对中位数再取中位数 我们在这里做了个证明，大致证明了这个最终的pivot，会呆在30%-70%的排名之间 于是这里出现了一条奇妙的式子 怎么算！代入！由于我们知道答案，我们直接猜猜O(n)]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Algorithm Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Study Less Study Smart]]></title>
    <url>%2F2019%2F02%2F12%2FStudyLessStudySmart%2F</url>
    <content type="text"><![CDATA[需要科学上网我换成b站的地址了]]></content>
      <categories>
        <category>内容分享</category>
      </categories>
      <tags>
        <tag>高效学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CMU15-213# Everything We Need]]></title>
    <url>%2F2019%2F02%2F09%2FCMU15-213_L0%2F</url>
    <content type="text"><![CDATA[Everything We Need CMU15-213 对应课程资料整合 这门课使用的教材为CSAPP 3th 神书配神课，据说这是CMU的镇校课之一… 愣是被我到处嫖凑齐了资料 总之打算长期给学完了，毕竟计算机原理是后期很多课的基础 比如操作系统 ,计算机网络 等等…好吧，可能计算机网络和我们专业更接近一点 站点资料 包括课程资料和博客参考 课程首页 这里选用2018 spring的课程主页 主要是课件比较齐全，包括习题课(recitation)讲义和每节课的lecture 以及每节课可以与ppt同步的video~ 如果需要的话可以在这里补齐 这里顺便解释一下recitation和tutorial是啥…//摘自知乎 Recitation是复习课，通常是高年级研究生来上。极少数情况下也会有讲师或者教授亲自主讲。只有非常重要的基础课程才会享受后者的待遇。Recitation是课堂内容的总结和延伸。 与之相关的概念是Tutorial，也就是习题课。一般是研究生助教上，以做题目为主。 博客参考 这是一个在CMU读完master的大神的博客 包括 每个章节的核心解读 以及 lab解析 //lab解析是最棒的有没有！ 以及我真的难以想象他是怎么做到看书这么快还能写博文的… 课程视频 youtube上有，其实建议直接可以看18 spring的官方给的video //真是诡异了，其他资料都是不让外校访问唯独这里可以… 文档资料 CSAPP 3th 干脆就直接读的原版，反正每一章节都有指定的导读，一次最多也就两三节。 Syllabus 这个很重要，包括导读以及学习的提纲，比如哪几节学完对应哪个lab，有利于系统学习。 lab 这门课的精髓，一定要自己把代码敲完！ 可以参考的资料有 lab-handout(code) README recitation 博客中lab部分的“题意解读”和“lab解答” lecture 很不幸，全都是PPT！不像CS161每个lecture好歹配个note… 不过还好，反正有书。 不过好有残念啊…过两天再找找]]></content>
      <categories>
        <category>计算机原理</category>
      </categories>
      <tags>
        <tag>CMU15-213</tag>
        <tag>Data Integration</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# ProblemSet1 Solution]]></title>
    <url>%2F2019%2F02%2F07%2FCS161_P1%2F</url>
    <content type="text"><![CDATA[PoblemSet1 Finished Solution 这里完成以下ProblemSet 1 对应章节大概是L1, L2 Exercise 看完了… 取 n0=5,c=8n_0=5, c=8n0​=5,c=8 ,满足 n&gt;n0n&gt;n_0n&gt;n0​ 时，2n+6&lt;8n2 \sqrt{n} + 6 &lt; 8 \sqrt{n}2n​+6&lt;8n​ A) 显然是 O(n)​O(n)​O(n)​ 。for循环总共n次，每次进行判断（以及一次赋值），但常数不重要，所以1op和2op不区分 B) 取 n0=20000,c=0.0001​n_0=20000, c=0.0001​n0​=20000,c=0.0001​ 。 原lst长度太小了，无法体现渐近，调整到了万级 C) 通过绘图的最远两个点大概估计了一下斜率。据此可算出 n=1015n=10^{15}n=1015 时大概需要花的时间 a) T(n)=2n+6, g(n)=n​T(n)=2\sqrt{n}+6,\ g(n)=\sqrt{n}​T(n)=2n​+6, g(n)=n​​ 取 n0=2,c=8​n_0=2, c=8​n0​=2,c=8​，显然满足 ∀n&gt;2，2n+6&lt;8n​\forall n&gt;2，2\sqrt{n}+6&lt;8\sqrt{n}​∀n&gt;2，2n​+6&lt;8n​​ ∴2n+6=O(n)​\therefore 2\sqrt{n}+6=O(\sqrt n)​∴2n​+6=O(n​)​ b) T(n)=n2, g(n)=nT(n)=n^2, \ g(n)=nT(n)=n2, g(n)=n 取 n0=2,c=1n_0=2, c=1n0​=2,c=1, 显然满足 ∀n&gt;2,2n2&gt;n\forall n&gt;2, 2n^2&gt;n∀n&gt;2,2n2&gt;n ∴n2=Ω(n)\therefore n^2=\Omega(n)∴n2=Ω(n) c) T(n)=log⁡2n, g(n)=ln⁡(n)T(n)=\log_2{n},\ g(n)=\ln(n)T(n)=log2​n, g(n)=ln(n) 取 n0=1,c1=log⁡2(e−1),c2=log⁡2(e+1)n_0=1, c_1=\log_2(e-1),c_2=\log_2(e+1)n0​=1,c1​=log2​(e−1),c2​=log2​(e+1) , 显然满足 ∀n&gt;1, log⁡2(e−1)ln⁡n&lt;log2n&lt;log⁡2(e+1)ln⁡n\forall n&gt;1,\ \log_2(e-1)\ln{n}&lt;log_2n&lt;\log_2(e+1)\ln n∀n&gt;1, log2​(e−1)lnn&lt;log2​n&lt;log2​(e+1)lnn ∴log⁡2(n)=Θ(ln⁡n)\therefore \log_2(n) = \Theta(\ln n)∴log2​(n)=Θ(lnn) d) 假设 4n=O(2n)​4^n = O(2^n)​4n=O(2n)​ 那么应有 ∀n&gt;n0,4n&lt;c2n​\forall n &gt; n_0, 4^n &lt; c 2^n​∀n&gt;n0​,4n&lt;c2n​ ,也就是 c&gt;2n​c &gt; 2^n​c&gt;2n​ 但是对于任一 n0​n_0​n0​​ ，满足这个条件的常数 c​c​c​ 都不存在 ∴4n≠O(2n)​\therefore 4^n \neq O(2^n)​∴4n̸​=O(2n)​ Problems (a)和(b)逻辑是没问题的，但是©是错的。© 最致命的一点是把O(1)O(1)O(1) 次&quot;操作&quot;和O(1)O(1)O(1) 的时间效率混淆。那按这么说，推到极端，所有程序都是O(1)O(1)O(1) 的，因为他们只调用了一次main。 题意简单而言，这题就是有n只青蛙，有好蛙和坏蛙，让他们相互评价，一次评价叫一次”toad-to-toad comparison“(此处因政治因素不对toad做直译，下称互评)。最终要求通过 O(n)O(n)O(n) 次互评判断所有好坏。 a) O(n2)O(n^2)O(n2) 的算法相当简单，对于任意一青蛙，让其与其他n-1只青蛙进行互评即可。 假设你是坏蛙，那么剩下至少一半的好蛙一定会说你是坏蛙。 假设你是好蛙，那么剩下至少一半的好蛙一定会说你是好蛙。 结果就是，”大多数蛙的判断是对的。“ b) 这里要求的函数功能是： 通过至多n/2次互评找出一个大小为m(m至多为n/2​n/2​n/2​)的子集 这个子集至少有一半是好蛙。 给我们n/2次操作，那不难相当分成两组对应互评这一策略，问题是怎样通过结果找到这个子集呢？ 我们可以建立一个逆向真值表，也就是通过互评结果看看有哪几种可能 互评结果 实际可能 TT TT or FF TF FT or FF FT TF or FF FF TF or FT or FF 可以看出，只有在结果为TT时，我们从{A, B}中取出的A与B的关系是确定的，要么都是好蛙，要么都是坏蛙。 我们从这些结果为TT的配对中，各拿一个组成新的子集(也就是大小为m的结果) 但我们这里需要证明 新子集中T比F多 配对的实际情况（不是结果）无非是TT，TF，FT，FF。我们现在无非需证明，哪怕在最坏的情况下，也就是所有FF都互相包庇判断为TT，其组数也不可能超过实际TT的组数。 由于无论是FT还是TF，去掉它们对于相对大小的判断都不会有影响，因此TT与FF组数的相对多少，恰好符合原集合中T与F的相对多少。因此TT组数必然小于FF组数。 c) 如果n是奇数怎么办？且最多只能互评 ⌊n/2⌋​\lfloor n/2 \rfloor​⌊n/2⌋​次 很简单，直接把多出来的那一个拿进来就好了，但这里问题又回到了证明子集中T的组数需要 ⌈m⌉​\lceil m \rceil​⌈m⌉​ 很显然，当推至”最坏的情况“时，我们需要多出来的那一个也为好蛙，那么为什么呢？ 因为在奇数的情况下，T的总数本身就是比F至少大1的，而FT和TF不影响相对多少，因此我们只用看FF全部说谎的极端情况，但是哪怕如此，T的总数大于F，虽然TT组数极端情况下和FF组数相等，但还剩一个啊！ 所以那个肯定是好蛙！ d) 假设这个函数是downsize(A)，A为蛙全体。 那么就运行这个函数直到A的size为1即可。最后这只留下的一定是一直好蛙。 e) Inductive Hypothesis: 在第k​k​k​次downsize后，mk≤n2k+1​m_k\le\frac{n}{2^k} +1​mk​≤2kn​+1​，且子集满足好蛙严格过半 Base Case: k=0k=0k=0 ，按照原题意，好蛙显然过半 Indective Step: 由downsize函数在n为奇数与偶数两种情况下的讨论，充其量m在全程均为奇数。 n2k+12k+⋯+12&lt;n2k+1\frac{n}{2^k} + \frac{1}{2^k} + \dots + \frac{1}{2} &lt; \frac{n}{2^k} + 1 2kn​+2k1​+⋯+21​&lt;2kn​+1 Conclusion: 当达到 m=1​m=1​m=1​ 时，由于size为奇数时严格过半，好蛙的个数至少为 ⌈1/2⌉=1​\lceil1/2\rceil=1​⌈1/2⌉=1​ ，必为好蛙 f）每一次downsize的效率为 O(mk)O(m_k)O(mk​), 而 mk≤n2k+1m_k\le\frac{n}{2_k}+1mk​≤2k​n​+1， ∴O(n)+O(m1)+O(m2)...+O(1)=O(n)\therefore O(n)+O(m_1)+O(m_2)...+O(1)=O(n)∴O(n)+O(m1​)+O(m2​)...+O(1)=O(n) g）你找到一个好蛙…让它在判断一遍所有蛙不就好了？反正它永远说真话啊！ 最后一次扫描，也就 O(n)O(n)O(n) ，加上之前的 O(n)O(n)O(n) ，还是 O(n)O(n)O(n) ​]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>solution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# Devide-and-conquer, Mergesort, and Big-O notation]]></title>
    <url>%2F2019%2F02%2F06%2FCS161_L2%2F</url>
    <content type="text"><![CDATA[对应CS161的Lecture2 Correctness and Running Time 算法中最关键的是两个问题 Correctness //Does it work? Running Time //How fast? 今天要讲的正是这两个问题，引入loop invariant这个概念 //这个概念非常重要，是证明算法正确性的利器 整体而言，我们依次会讲解以下两个算法 InsertionSort MergeSort 我们会从一开始提到的两个角度对这两个算法进行分析 InsertionSort 注意这里是python代码，range(l,r)和A[l:r]均是前闭后开 流程非常简单，每一次取已排部分末尾，像夹娃娃机一样，用current夹起来，其他元素让出位子，然后用current放上去，形成新的有序序列。 Correctness 事实上从过程的语言叙述中，我们就可以看出这个算法的一部分正确性证明了。 但是显然这样证明不严谨，这里我们引入loop invariant （下称 LI ） 我们这里摘录一下Wikipedia中的部分解释 LI is a property of a program loop that is true before (and after) each iteration. Knowing its invariant(s) is essential in understanding the effect of a loop. 证明整个过程需要分四步，我们会在下面演示 (注意以下步骤中A下标从0开始，而第 i=ni=ni=n 次循环的意义也取决于i​i​i​的循环初始值) Inductive Hypothesis: 在第 i=ni=ni=n 次循环之后，A[:n+1]A[:n+1]A[:n+1] 有序 Base case: 第i=0i=0i=0循环后(也就是 i=1i=1i=1 开始前)， A[:1]A[:1]A[:1] 只有1个，显然有序 Inductive step: 假如现在第i−1i-1i−1次循环结束，A[:i]A[:i]A[:i] 有序；那么在经过第 iii 次循环的插入操作后（内部 jjj 循环）, A[:i+1]A[:i+1]A[:i+1] 如下A[0],A[1],…,A[j∗],…,A[i−1],A[i]A[0] , A[1] , \dots , A[j^*] , \dots , A[i-1] , A[i] A[0],A[1],…,A[j∗],…,A[i−1],A[i] 其中 A[j∗]A[j^*]A[j∗] 以左均为小于它的元素，以右均为大于它的元素，且左右各自相对有序，则显然现在经过了第 iii 次循环，A[:i+1]A[:i+1]A[:i+1] 整体有序 Conclusion: 当循环终止时，也就是第 len(A)−1len(A)-1len(A)−1 次循环后，A[:len(A)]A[:len(A)]A[:len(A)] 有序，目标达成 至此我们的算法证明完毕。 在这里我们可以获得的一点感悟是，读懂循环的所谓目的，其实流程无非是找到Inductive Hypothesis，并且让Conclusion能够为我们的实现目标。 Running Time 这里我们不难看到，若序列长为nnn，外层循环nnn次，而内层jjj最多循环iii次，那么精确一些的结果是 1+2+⋯+n=n(n+1)21+2+\dots+n=\frac{n(n+1)}{2} 1+2+⋯+n=2n(n+1)​ 但是实际上我们并不care这个常数，因此其实是直接当做n2n^2n2来看。至于原因，后面会提。 MergeSort 绿色处为哨兵！ Correctness 这里需要先证明一个recursion invariant 不难看出算法分析中非常喜欢invariant，也就是被操作对象维持一定的property，这个property最后可以帮助我们证明算法的正确性。//比如在这里，对象就是无序序列，目标就是让它变有序。 也就是Merge以后的结果必须要有序，所以最终证明变成了证明 Merge可以正确执行它的工作。 所以接下来问题又回到了loop invariant，继续开始我们的四步曲吧！ Inductive Hypothesis: (1)在第kkk次循环后，A[p:(k+1)]A[p:(k+1)]A[p:(k+1)] 有序 (2)i,ji,ji,j 指向对应组中最小的元素。 Base case: 当k=p−1k=p-1k=p−1时，A[p:p]A[p:p]A[p:p] 为空集，(1)显然成立。i=j=1,显然也符合(2) Inductive step: 假如现在第 k−1k-1k−1 次循环后，A[p:k]A[p:k]A[p:k] 有序,且 L[i]L[i]L[i] 和 R[j]R[j]R[j] 最小。则经过第 kkk 次循环后，L[i]L[i]L[i] 与 R[j]R[j]R[j] 取较大值加入 A[k]A[k]A[k] ,不妨假设现在取得是L[i]L[i]L[i] ,那么经过 i+=1i+=1i+=1 之后，显然 L[i]L[i]L[i] 仍是 LLL 剩下中最小，RRR 中情况不变，因此显然(2)成立；而 A[:k+1]A[:k+1]A[:k+1] 在 A[:k]A[:k]A[:k] 有序的基础上，满足 A[:k]A[:k]A[:k] 所有元素均小于 A[k]A[k]A[k] ,且 A[:k]A[:k]A[:k] 内部有序，A[k+1]A[k+1]A[k+1] 显然有序。 Conclusion: 当 k=rk=rk=r 循环完成时, A[p:(r+1)]A[p:(r+1)]A[p:(r+1)] 有序，达成目标。 Running Time 证明分支算法的时间效率，一般利用递推式，和画出以上的recursion tree两种 似乎实质是一样的，只不过画出树来稍微直观一些（虽然递推明明也很直观啊） 首先我们分析一下Merge函数的效率，很容易得到为 O(n)O(n)O(n) //具体分析过程只要数操作数就行了，因为MERGE函数并没有内部调用只有顺序执行 分析完Merge函数，我们要开始分析整个MergeSort \begin{align*} T(n) &amp;= T(n/2)+T(n/2)+O(n)\\ &amp;=2\cdot T(n/2)+O(n) \end{align*} 由上面画出的这颗树也很容易看到，对于第t层而言，任务有 2t2^t2t 个，每个规模为 n2t​\frac{n}{2^t}​2tn​​ 那么对于单层而言，他们“自己做的活”(不包括交给子任务的) \begin{align*} Work\ at\ Level\ i &amp;= (number\ of\ subproblems) \cdot (work\ per\ subproblem)\\ &amp;= 2^i \cdot O(\frac{n}{2^i})\\ &amp;=O(n) \end{align*} 而层数共有 log⁡2n\log_2{n}log2​n 层，于是很容易得到 \begin{align*} Total\ Work &amp;= (Work\ per\ level)\cdot(number\ of\ levels) \\ &amp;=O(n)\cdot \log_2(n)\\ &amp;=O(n\log_2{n}) \end{align*} 于是我们得到了最终答案。 实际上对于这类子问题规模相同的算法，我们可以用master method进行计算。 在这里实际上已经有了一些master method的影子，需要好好品味。 Guiding Principle 对于算法的运行快慢的评估，我们究竟应该用什么指标去评价？这里会给出答案 Worst-Case Analysis 既然是算法，自然要考虑最坏的情况，毕竟做好最坏的打算肯定是没错的（严肃脸） 我们可以想象我们在进行一场对垒，对手希望用最恶心的数据打败我们 //让我们的算法跑的更慢 那么我们需要保证的是，哪怕他给出最恶心的数据，我也能hold住 好吧说白了就是ACM里的抗hack… Asymptotic Analysis 我们另一个关心的指标是渐近效率。用人话讲，就是当nnn非常大的时候，它会以怎样的趋势增长。 这实际上正是我们忽略常数的原因，因为我们当nnn很大时它的增长趋势并不会怎么受常数影响。 当然！这不代表我们认为常数真的不重要，我们之所以在此忽略常数是因为 当我们用理论流程和伪代码去表示算法时，讨论常数其实是没有意义的。因为你压根不知道常数是多少！这取决于你的机器和使用的语言等。 Asymtotic Notation 事实上渐近是一个数学概念，只是算法里用到比较多而已。 既然如此，我们不妨用数学符号定义一下这个概念。 &quot;Big-Oh&quot; Notation T(n)=O(f(n))⇔∃c,n0&gt;0. ∀n≥n0,0≤T(n)≤c⋅f(n)T(n)=O(f(n)) \Leftrightarrow \exists c,n_0&gt;0.\ \forall n \ge n_0, 0\le T(n) \le c\cdot f(n) T(n)=O(f(n))⇔∃c,n0​&gt;0. ∀n≥n0​,0≤T(n)≤c⋅f(n) 从表达式中不难看出，这个f(n)f(n)f(n) 代表渐近上界 &quot;Big-Omega&quot; Notation T(n)=Ω(f(n))⇔∃c,n0&gt;0. ∀n≥n0,0≤c⋅f(n)≤T(n)T(n)=\Omega(f(n)) \Leftrightarrow \exists c,n_0&gt;0.\ \forall n \ge n_0, 0\le c\cdot f(n) \le T(n) T(n)=Ω(f(n))⇔∃c,n0​&gt;0. ∀n≥n0​,0≤c⋅f(n)≤T(n) 类似地，这个f(n)​f(n)​f(n)​ 代表渐近下界 &quot;Big-Theta&quot; Notation T(n)=Θ(f(n))⇔∃c1,c2,n0&gt;0. ∀n≥n0,0≤c1f(n)≤T(n)≤c2f(n)T(n)=\Theta(f(n)) \Leftrightarrow \exists c_1,c_2,n_0&gt;0.\ \forall n \ge n_0, 0\le c_1f(n) \le T(n) \le c_2 f(n) T(n)=Θ(f(n))⇔∃c1​,c2​,n0​&gt;0. ∀n≥n0​,0≤c1​f(n)≤T(n)≤c2​f(n) 当O(f(n))=Ω(f(n))​O(f(n))=\Omega(f(n))​O(f(n))=Ω(f(n))​ 时，则称 T(n)=Θ(f(n))​T(n)=\Theta(f(n))​T(n)=Θ(f(n))​ 当然，上面给出的这个表达其实也是等价的。 最后给出一个比较直观的图，描述了 Θ, Ω, O​\Theta,\ \Omega,\ O​Θ, Ω, O​ 的关系 Algorithm Implementation 这里我们再顺便把几种sort算法用python实现一下 这里尤其要注意，MergeSort中的哨兵，对于边界条件的控制作用（卡死不让访问无效内存） BubbleSort InsertionSort SelectionSort MergeSort 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566INF = 0xffffffffffffffdef BubbleSort(A): for i in range(len(A)-1): for j in range(len(A)-1, i, -1): if(A[j]&lt;A[j-1]): A[j], A[j-1] = A[j-1], A[j] return Adef InsertionSort(A): for i in range(1, len(A)): current = A[i] j = i-1 while (j&gt;=0 and A[j]&gt;=current): A[j+1] = A[j] j -= 1 A[j+1] = current return Adef SelectionSort(A): for i in range(len(A)-1): minx = INF minPos = -1 for j in range(i, len(A)): if(A[j] &lt; minx): minx = A[j] minPos = j A[i], A[minPos] = A[minPos], A[i] return Adef Merge(L,R): m = len(L) + len(R) #哨兵！ L += [INF] R += [INF] S = [] i = 0 j = 0 for k in range(m): if(L[i]&gt;R[j]): S += [R[j]] j += 1 else: S += [L[i]] i += 1 return Sdef MergeSort(A): r = len(A)-1; if (0 &gt;= r): return A mid = (0+r)//2 L = MergeSort(A[:(mid+1)]) R = MergeSort(A[(mid+1):]) return Merge(L, R)if __name__=="__main__": arr = [4,1,3,9,10,7,2,0] print("Origin Arr\t",arr) print("MergeSort\t",MergeSort(arr)) ans1 = [4,1,3,9,10,7,2,0] print("BubbleSort\t",BubbleSort(arr)) arr = [4,1,3,9,10,7,2,0] print("InsertionSort\t",InsertionSort(arr)) arr = [4,1,3,9,10,7,2,0] print("SelectionSort\t",SelectionSort(arr))]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Algorithm Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《认知天性》小感]]></title>
    <url>%2F2019%2F02%2F05%2Fcognitive_nature%2F</url>
    <content type="text"><![CDATA[还没更完，有关“穿插学习”的内容仍需跟进 这本书其实之前看了以后一直想讲一讲，真的纠正了自己之前很多错误的观念。 认知天性这样的翻译似乎有点莫名其妙，但是这里看一下它的英文书名 "MAKE IT STICK: THE SCIENCE OF SUCCESSFUL LEARNING" 正如它的书名一样，学习的关键在于STICK，但这里又不能简单理解为“记住”，而是在深刻记忆的基础上，对知识进行化用，直到让知识嵌入自己的心智模型。 之前我分享读后感或者读书笔记的方式，只是沿用了《如何阅读一本书》中的观点，也就是 先掌握整本书的叙述框架(目录+粗读)，再在基本框架的基础上进行填充(精读)。 但在具体的执行上，似乎显得有些“目录主义”，再加上自己有些强迫症，于是本来的读书笔记成为了“抄书大会”。 于是最后的结果是，我以为写完笔记自己就真的懂了。 停止对你的课本“浓妆艳抹” 然后今天的这本书告诉我，全tm是错觉。 现在的学生喜欢在书上画满各种花花绿绿的线，这里画个框，那里圈个圈。以为自己抓住了重点。 但事实是，你以为自己抓住了重点。但是你的大脑也罢工了。 你的大脑告诉自己，这个东西突出出来，我的任务已经完成了，我不需要记住它了。 这其实跟现在的资料搜集狂是有一些相似的，收藏夹里都快爆炸了，但是真的自己吸收过得干货根本没有多少。 为什么呢？因为你把它扔进收藏夹里的那一刻，你就是在告诉自己 “反正我以后会去看它的。” 事实上你没有…你还是在不断搜集资料的路上，但是那些好的经验与方法，你一个也没吸收。 这就是为什么韩寒说 我们听过很多道理，却依然过不好这一生。 废话！你听过很多道理，却一个都没记住，一个也没实践，能过好就怪了。 很多人觉得听到某句话时，真的是触动灵魂！决定把这句话当做自己的motto。 然后第二天起床，什么？昨天我干了什么呢。 人就是这么废掉的。 学习需要“刻意” 究其原因，是因为你太习惯舒适了，而学习本就是痛苦的。 人天生易忘，但偏偏学习的基础就是记忆。而有些人就是觉得做完笔记把书扔一边，自己就能学会所有的东西。 当然，有的人很勤奋，他们一遍又一遍的读书，觉得读的越来越顺对书越来越熟，就是掌握了知识。 但是合上书呢? 这里樊登老师给出了一个很形象的比喻：捋绳子。 知识就像绳子，你的每一次不厌其烦地看书，就像不断地去捋绳子 结果每次都是从头捋到尾，脑子里啥都没留下 但是记忆是要给这绳子打结的！ 那到底怎么要才能记得住？考试！ 用检测帮助记忆 可能大家一听到考试就讨厌，那是因为考试往往和成绩挂钩。而成绩…太被看重了。 但是这里需要清楚的一点是，考试本身是有助于学习的。我们看一下考试的一种理解 在【没有参考资料的情况下独立完成】的【知识检索】 核心，其实简洁一点说，就是**“强迫检索”** 强迫，避免了自欺欺人，也就是在你明明都还没想起来的情况下看到课本，然后当个“秒懂”的“事后诸葛亮”。 而检索，才是学习的核心。当然，这里的检测是需要有反馈的，不然你学到的东西是对是错都无法确定，用检测去纠正自己的误解，也是检测很必要的一面。 但是正如我前面提到的，大家都讨厌考试，更讨厌在考试里犯错，因为犯错往往意味着考砸。所以有些人，会在自我检测的时候，当一个对自己极为宽松的“良师”，模棱两可都算对。那么真的在考试的时候，你是指望每个老师都这样对待你这份模棱两可的试卷吗？ 这里其实在《刻意练习》和《成长型思维》两本书中都有提到 重在过程 越盯着结果不放，反而越是无力进步。 observe-&gt;practice-&gt;progress 这才是正确的学习之道 从检索说开去 从个人的理解而言，其实检索和之前自己读到过的很多东西都是有关系的。 比如《如何阅读一本书》中提到的积极阅读，以及在英语学习中的“表达越用越熟”的说法，其实都是有关系的。从通俗一点的角度来讲，我觉得可以概括为一句话 脑子越用越活 这某种程度上也是我想着开始写博客的目的。 之前听说过有人会在英语的学习中选择一种输出倒逼输入的方式，也就是强行要你写东西，因为写不出来，就会想着去查各种表达，最后完成一篇文章。 这种逆向学习的方法在“知识搜集”上显得有一些功利，而且可能学到的东西显得有点碎片化。毕竟你一次也就用一个表达，比如 坚持立场 = dig one’s heels in，然后在你脑子里，这个表达就只是一个孤立的点，也就很难让你想到它。就像一座孤岛，你在海上找，很难找到。 如果说孤岛很难寻找，那么群岛呢？ 你查了dig one’s heels in，为什么不查查stand up for， 为什么不查查stand one’s ground。maintain the position 和 fight one’s corner和其他的有什么区别？如果区别不大，能不能一起用？ 查了 坚持立场，你问问自己，你能想到坚持常常和什么搭配吗？firm, dogged, strong-willed, tough, persistent? 它们的theraurus里的rigid, stubborn, stern, harsh？似乎和坚持有些不一样。但总归有些共同点吧？ 于是每一次查字典，一片一片地查，你每一次获得的都是一个群岛。 好处是什么呢？你找到群岛的一角，那么离你想要的那个孤岛，也就不远了。 学习不就是这样吗？主动建立连接，你去找知识，别等着知识去找你。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>高效学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[#CS161# Do you know how to multiply integers?]]></title>
    <url>%2F2019%2F02%2F05%2FCS161_L1%2F</url>
    <content type="text"><![CDATA[对应CS161的Lecture 1 Method to multiply integers 我天真的以为L1不会有什么东西的，看了一眼居然出了overview还真有东西，特此补上。 这一节大概讲了那么两个东西 Graduate-School Integer Multiplication Karatsuba Integer Multiplication //别问我这名字为什么这么鬼畜😳 Graduate-School Integer Multiplication 不分析了…就是从小做到大的竖式进位乘法 真的有什么要说的话，这节课里提到了一个看上去很Tricky的方法 把数字各自分块成两块: x=10n/2+b, y=10n/2+dx∗y=10nac+10n/2(ad+bc)+bdx=10^{n/2}+b,\ y=10^{n/2}+d\\ x*y =10^nac+10^{n/2}(ad+bc)+bd x=10n/2+b, y=10n/2+dx∗y=10nac+10n/2(ad+bc)+bd 但是事实上，在这里如果我们分别对ac, ad, bc, bdac,\ ad, \ bc, \ bdac, ad, bc, bd进行计算，那么实际效率并没有提升 T(n)=4T(n/2)+O(n)T(n)=4T(n/2)+O(n) T(n)=4T(n/2)+O(n) 在后面利用Master Method进行分析的结果，容易得到T(n)=O(n2)T(n)=O(n^2)T(n)=O(n2) Karatsuba Integer Multiplication 事实上，在这里虽然x∗yx*yx∗y已经是一个&quot;Information efficient&quot;的表示，但并不意味着它&quot;Computation efficient&quot; 既然乘法对于我们而言是一个代价较大的操作，那么我们能不能尽量考虑减少乘法。（哪怕增多加减法） ad+bc=(a+b)(c+d)−ac−bdad+bc=(a+b)(c+d)-ac-bd ad+bc=(a+b)(c+d)−ac−bd 似乎有一些神奇的事情发生了，原本两次乘法的操作似乎现在用一次就可以完成了呢！ 于是原本 T(n)T(n)T(n)，也就是Worst Running Time，的递推式似乎也发生了变化！ T(n)=3T(n/2)+O(n)T(n)=3T(n/2)+O(n) T(n)=3T(n/2)+O(n) 我们再次利用Master Method，得到的新的结果T(n)=O(nlog⁡23)≈O(n1.6)T(n) = O( n^{\log_2{3}} ) \approx O(n^{1.6})T(n)=O(nlog2​3)≈O(n1.6) 似乎的确是个略优的算法呢！ 然后我动手实现了一下这个诡异的算法 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom multHelpers import *#makeInt(x)#getDigits(X)#def multABunch(myFn, nVals, numTrials=20#myFn - Function to be used in multiplication#nVals - List include different length for data generator#numTrials - how many times of testing for each length#x - List, Y - Intdef karatsubaMult(X, Y): x = getDigits(X) y = getDigits(Y) nx = len(x) ny = len(y) n = max(nx, ny) x = [0 for i in range(n-nx)] + x y = [0 for i in range(n-ny)] + y return karatsubaMult_Helper(x, y)def karatsubaMult_Helper(x, y): n = len(x) if (n!=1 and n%2==1): x = [0] + x y = [0] + y n = len(x) if(n==1): return x[0]*y[0] a = x[:n//2] b = x[n//2:] c = y[:n//2] d = y[n//2:] termAC = karatsubaMult_Helper(a, c) termBD = karatsubaMult_Helper(b, d) termMID = karatsubaMult(makeInt(a)+makeInt(b), makeInt(c)+makeInt(d))-termAC-termBD ret = (10**n)*termAC+(10**(n//2))*termMID+termBD #print(x, y, termAC, termBD, termMID, ret) return retif __name__=="__main__": X = 212342131398 Y = 25415243554767 print(X*Y) print(karatsubaMult(X, Y))]]></content>
      <categories>
        <category>基本算法</category>
      </categories>
      <tags>
        <tag>CS161</tag>
        <tag>Karatsuba Algorithm</tag>
      </tags>
  </entry>
</search>
